# VQA-FusionNet
VQA-FusionNet is a multimodal Visual Question Answering (VQA) framework that fuses image and text features using a combination of Vision Transformer (ViT) and BERT. It enables models to interpret an image and answer natural language questions about it â€” bridging the gap between visual understanding and language comprehension.

# Visual Question Answering (VQA)

This project implements a VQA system using a Vision Transformer (ViT) and BERT for multimodal fusion.

## Structure
- `notebooks/`: Jupyter notebooks for experimentation
- `src/`: Source code including models, dataset, training loop, and configuration
- `data/`: Dataset (not included in repo)
- `outputs/`: Model checkpoints

## How to Run

```bash
pip install -r requirements.txt
